<!doctype html><html lang=en><head><title>Regularization in Machine Learning // Affan Shoukat</title><link rel="shortcut icon" href=../../favicon.ico><meta charset=utf-8><meta name=generator content="Hugo 0.83.1"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Dr. Affan Shoukat"><meta name=description content><link rel=stylesheet href=https://affans.github.io/personal/css/main.min.88e7083eff65effb7485b6e6f38d10afbec25093a6fac42d734ce9024d3defbd.css><meta name=twitter:card content="summary"><meta name=twitter:title content="Regularization in Machine Learning"><meta name=twitter:description content="What is regularization in machine learning algorithms? Regularization is a modification to a learning algorithm that aims to reduce its generalization error by not its training error."><meta property="og:title" content="Regularization in Machine Learning"><meta property="og:description" content="What is regularization in machine learning algorithms? Regularization is a modification to a learning algorithm that aims to reduce its generalization error by not its training error."><meta property="og:type" content="article"><meta property="og:url" content="https://affans.github.io/personal/post/reg-glmnet/"><meta property="article:section" content="post"><meta property="article:published_time" content="2021-05-16T00:00:00+00:00"><meta property="article:modified_time" content="2021-05-16T00:00:00+00:00"></head><body><header class=app-header><a href=https://affans.github.io/personal><img class=app-header-avatar src=../../images/profile.jpg alt="Dr. Affan Shoukat"></a><h1>Affan Shoukat</h1><nav class=app-header-menu><a class=app-header-menu-item href=../../posts/>Posts</a>
-
<a class=app-header-menu-item href=../../about/>About</a></nav><p>Personal and Academic</p></header><main class=app-container><article class=post><header class=post-header><h1 class=post-title>Regularization in Machine Learning</h1><div class=post-meta><div><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar"><title>calendar</title><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>May 16, 2021</div><div><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock"><title>clock</title><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>4 min read</div></div></header><div class=post-content><p>What is <em>regularization</em> in machine learning algorithms? Regularization is a modification to a learning algorithm that aims to reduce its generalization error by not its training error. This technique is often used for very high-dimensional data in which the number of features (or predictors, explanatory variables) exceeds the sample sample size.</p><h2 id=prerequisites-and-context>Prerequisites and Context</h2><p>Our context for this post is linear regression where the target variable $y$ is explained by $n$ features $X = (x_1, x_2, \ldots, x_n)$, with corresponding coefficients $\beta = (\beta_1, \beta_2, \ldots, \beta_n)$. Recall the cost function (with respect to $\beta$) of the linear regression algorithm is given by $\Vert y - X\beta \Vert_2^2$ over $N$ data points. Regression by OLS then estimates the coefficients $\beta$ (and the intercept) by minimizing the squared prediction errors across the training data. To increase model accuracy, one tries to fit as many datapoints by moving the model from linear to quadratic to polynomial. However, as model complexity grows, higher degree model and large coefficients increase the variance significantly leading to overfitting.</p><p>In order to avoid this overfitting scenario and increase the model robustness, we can do two things: (1) shrink the coefficients in the model and (2) get rid of high degree polynomials features in the model. This is precisely known as <em>regularization</em>.</p><p>However, by shrinking the coefficients of an already optimized $\beta$, we lose model accuracy. To balance the accuracy levels, we add Bias &mldr; part of the model that is not dependent on feature data. By adding Bias in the model, we consequently reduce the variance. In other words, as input variables are changed, the model&rsquo;s prediction changes less than it would have without the bias.</p><p>Ridge and Lasso regularization work by adding a new term to the cost function used to derive the regression formula.</p><h2 id=ridge-and-lasso-regression>Ridge and Lasso Regression</h2><p>Ridge Regression finds coefficients by minimizing the sum of squared error loss, subject to $L_2$ norm constraint on the coefficients. That is,
$$\hat{\beta}(\lambda) = \operatorname*{argmin}_\beta \Vert y - X\beta \Vert_2^2 + \lambda \Vert\beta\Vert_2^2$$
where $X$ is the matrix of features (or variables), $y$ is a response vector, $\beta$ is the coefficient vector, and $\lambda$ is a **positive regularization parameter** or a tuning parameter. (Note that $\lambda$ is a hyperparameter, otherwise the gradient descent method would simply set it to zero). As the coefficients $\beta$ vary in the minimization process, each particular combination value of $\beta$ will generate a bias output . The size of the output depends on the total magnitude of all the coefficients and including extra features will result in increased values.</p><p>Sometimes geometry helps. Consider an example with two coefficients, say $\beta = (\beta_1, \beta_2)$ and lets suppose $\lambda = 1$. There can be multiple values that generate the same bias such as $(1, 0), (-1, 0), (0, -1)$. In the $L_2$ norm, the various combinations of $\beta_1$ and $\beta_2$ that generate the same bias form a circle.</p><p><img src=../../images/ridge_l2_norm.png alt="l2 norm"></p><p>In this figure, the black circle are the constraint regions $\beta_1^2 + \beta_2^2 &lt; t$ for some $t$ and the colored circles are the contours of the errors from the Gradient Descent. The additional bias term wants to pull the values of $\beta_1, \beta_2$ somewhere on black circle, while gradient descent is trying to travel to the global minimum represented by the dot. This balancing act eventually settles on the intersection indicated by the red cross.</p><p>The Lasso Regression is similar, except it penalizes the size of the $L_1$ norm of the cofficients, that is
$$\hat{\beta}(\lambda) = \operatorname*{argmin}_\beta \Vert y - X\beta \Vert_2^2 + \lambda \Vert\beta\Vert_1$$ where, again, $\lambda$ is a positive tuning parameter. The fundamental difference between Ridge and Lasso is that the $L_1$ norm constraint yields a sparse solution. By assigning zero coefficients to a subset of variables, the Lasso provices automatic feature selection, where the regularization parameter $\lambda$ controls the feature selection.</p><p>From a geometric view, we have</p><p><img src=../../images/lasso_l1_norm.png alt="l1 norm"></p><p>Why do L1 norms achieve sparse solutions? See this Math.ME post (<a href=https://stats.stackexchange.com/questions/45643/why-l1-norm-for-sparse-models)>https://stats.stackexchange.com/questions/45643/why-l1-norm-for-sparse-models)</a>.</p><p>Stable and accurate regression/classification model will require some sort of penalization on the L1 or the L2 norm of the coefficients.
This L1/L2 &ldquo;regularization&rdquo; brings in many technical advantages.</p><p>The simplest non-mathematical explanation is that For L2: Penalty term is squared,so squaring a small value will make it smaller. We don&rsquo;t have to make it zero to achieve our aim to get minimum square error, we will get it before that. For L1: Penalty term is absolute,we might need to go to zero as there are no catalyst to make small smaller.</p><p>Underfitting = High Bias
Overfitting = High Variance
Higher degree model and large coefficients increase the variance significantly leading to Overfitting</p></div><div class=post-footer></div></article></main></body></html>