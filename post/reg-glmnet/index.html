<!doctype html><html lang=en><head><title>Regularization Techniques in Regression Models // Affan Shoukat</title><link rel="shortcut icon" href=../../favicon.ico><meta charset=utf-8><meta name=generator content="Hugo 0.83.1"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Dr. Affan Shoukat"><meta name=description content><link rel=stylesheet href=https://affans.github.io/personal/css/main.min.88e7083eff65effb7485b6e6f38d10afbec25093a6fac42d734ce9024d3defbd.css><meta name=twitter:card content="summary"><meta name=twitter:title content="Regularization Techniques in Regression Models"><meta name=twitter:description content="What is regularization in regression models? Regularization is a modification to a learning algorithm that aims to reduce its generalization error by not its training error."><meta property="og:title" content="Regularization Techniques in Regression Models"><meta property="og:description" content="What is regularization in regression models? Regularization is a modification to a learning algorithm that aims to reduce its generalization error by not its training error."><meta property="og:type" content="article"><meta property="og:url" content="https://affans.github.io/personal/post/reg-glmnet/"><meta property="article:section" content="post"><meta property="article:published_time" content="2021-05-16T00:00:00+00:00"><meta property="article:modified_time" content="2021-05-16T00:00:00+00:00"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.css integrity=sha384-t5CR+zwDAROtph0PXGte6ia8heboACF9R5l/DiY+WZ3P2lxNgvJkQk5n7GPvLMYw crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.js integrity=sha384-FaFLTlohFghEIZkw6VGwmf9ISTubWAVYW8tG8+w2LAIftJEULZABrF9PPFv+tVkH crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/contrib/auto-render.min.js integrity=sha384-bHBqxz8fokvgoJ/sc17HODNxa42TlaEhB+w8ZJXTc2nZf1VgEaFZeZvT4Mznfz0v crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script></head><body><header class=app-header><a href=https://affans.github.io/personal><img class=app-header-avatar src=../../images/profile.jpg alt="Dr. Affan Shoukat"></a><h1>Affan Shoukat</h1><nav class=app-header-menu><a class=app-header-menu-item href=../../posts/>Posts</a>
-
<a class=app-header-menu-item href=../../about/>About</a></nav><p>Personal and Academic</p></header><main class=app-container><article class=post><header class=post-header><h1 class=post-title>Regularization Techniques in Regression Models</h1><div class=post-meta><div><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar"><title>calendar</title><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>May 16, 2021</div><div><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock"><title>clock</title><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>6 min read</div></div></header><div class=post-content><p>What is <em>regularization</em> in regression models? Regularization is a modification to a learning algorithm that aims to reduce its generalization error by not its training error. This technique is often used for very high-dimensional data in which the number of features (or predictors, explanatory variables) exceeds the sample sample size.</p><h2 id=prerequisites-and-context>Prerequisites and Context</h2><p>Our context for this post is linear regression where the target variable $y$ is explained by $n$ features $X = (x_1, x_2, \ldots, x_n)$, with corresponding coefficients $\beta = (\beta_1, \beta_2, \ldots, \beta_n)$. Recall the cost function (with respect to $\beta$) of the linear regression algorithm is given by $\Vert y - X\beta \Vert_2^2$ over $N$ data points. Regression by OLS then estimates the coefficients $\beta$ (and the intercept) by minimizing the squared prediction errors across the training data. To increase model accuracy, one tries to fit as many datapoints by moving the model from linear to quadratic to polynomial. However, as model complexity grows, higher degree model and large coefficients increase the variance significantly leading to overfitting.</p><p>In order to avoid this overfitting scenario and increase the model robustness, we can do two things: (1) shrink the coefficients in the model and (2) get rid of high degree polynomials features in the model. This is precisely known as <em>regularization</em>.</p><p>However, by shrinking the coefficients of an already optimized $\beta$, we lose model accuracy. To balance the accuracy levels, we add Bias &mldr; part of the model that is not dependent on feature data. By adding Bias in the model, we consequently reduce the variance. In other words, as input variables are changed, the model&rsquo;s prediction changes less than it would have without the bias.</p><p>Ridge and Lasso regularization work by adding a new term to the cost function used to derive the regression formula.</p><h2 id=ridge-and-lasso-regression>Ridge and Lasso Regression</h2><p>Ridge Regression finds coefficients by minimizing the sum of squared error loss, subject to $L_2$ norm constraint on the coefficients. That is,
$$\hat{\beta}(\lambda) = \operatorname*{argmin}_\beta \Vert y - X\beta \Vert_2^2 + \lambda \Vert\beta\Vert_2^2$$
where $X$ is the matrix of features (or variables), $y$ is a response vector, $\beta$ is the coefficient vector, and $\lambda$ is a **positive regularization parameter** or a tuning parameter. (Note that $\lambda$ is a hyperparameter, otherwise the gradient descent method would simply set it to zero). As the coefficients $\beta$ vary in the minimization process, each particular combination value of $\beta$ will generate a bias output . The size of the output depends on the total magnitude of all the coefficients and including extra features will result in increased values.</p><p>Sometimes geometry helps. Consider an example with two coefficients, say $\beta = (\beta_1, \beta_2)$ and lets suppose $\lambda = 1$. There can be multiple values that generate the same bias such as $(1, 0), (-1, 0), (0, -1)$. In the $L_2$ norm, the various combinations of $\beta_1$ and $\beta_2$ that generate the same bias form a circle.</p><p><img src=../../images/ridge_l2_norm.png alt="l2 norm"></p><p>In this figure, the black circle are the constraint regions $\beta_1^2 + \beta_2^2 &lt; t$ for some $t$ and the colored circles are the contours of the errors from the Gradient Descent. The additional bias term wants to pull the values of $\beta_1, \beta_2$ somewhere on black circle, while gradient descent is trying to travel to the global minimum represented by the dot. This balancing act eventually settles on the intersection indicated by the red cross.</p><p>The Lasso Regression is similar, except it penalizes the size of the $L_1$ norm of the cofficients, that is
$$\hat{\beta}(\lambda) = \operatorname*{argmin}_\beta \Vert y - X\beta \Vert_2^2 + \lambda \Vert\beta\Vert_1$$ where, again, $\lambda$ is a positive tuning parameter. From a geometric view, we have</p><p><img src=../../images/lasso_l1_norm.png alt="l1 norm"></p><p>The fundamental difference between Ridge and Lasso is that the $L_1$ norm constraint yields a <a href=https://stats.stackexchange.com/questions/45643/why-l1-norm-for-sparse-models>sparse solution</a>. In other words, the point of intersection between $L_1$ norm and the gradient descent contour to converge near the axes, and thus assigning the coefficient value to zero. By assigning zero coefficients to a subset of variables, the Lasso provices automatic feature selection, where the regularization parameter $\lambda$ controls the feature selection.</p><h2 id=code-sample>Code Sample</h2><p>Here is sample code to run Lasso Regression in Julia, using the package <code>GLMNet</code>.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=color:#75715e># replicates the R-GLM analysis from https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-019-0681-4</span>
<span style=color:#66d9ef>using</span> Flux
<span style=color:#66d9ef>using</span> Gnuplot
<span style=color:#66d9ef>using</span> GLMNet
<span style=color:#66d9ef>using</span> CSV
<span style=color:#66d9ef>using</span> DataFrames
<span style=color:#66d9ef>using</span> PrettyTables
<span style=color:#66d9ef>using</span> Random
<span style=color:#66d9ef>using</span> LinearAlgebra

_data <span style=color:#f92672>=</span> DataFrame(CSV<span style=color:#f92672>.</span>File(<span style=color:#e6db74>&#34;regularized_glm_data.csv&#34;</span>)) 
filter!(r <span style=color:#f92672>-&gt;</span> r<span style=color:#f92672>.</span>bare_nuclei <span style=color:#f92672>!=</span> <span style=color:#e6db74>&#34;NA&#34;</span>, _data) 
_data<span style=color:#f92672>.</span>bare_nuclei <span style=color:#f92672>=</span> parse<span style=color:#f92672>.</span>(<span style=color:#66d9ef>Int64</span>, _data<span style=color:#f92672>.</span>bare_nuclei)
_data <span style=color:#f92672>|&gt;</span> pretty_table

<span style=color:#75715e># sampling the test/training set </span>
index <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span><span style=color:#f92672>:</span>nrow(_data) 
testindex <span style=color:#f92672>=</span> rand(index, <span style=color:#66d9ef>Int64</span>(trunc(length(index)<span style=color:#f92672>/</span><span style=color:#ae81ff>3</span>)))
testdf <span style=color:#f92672>=</span> _data[testindex, <span style=color:#f92672>:</span>]
traindf <span style=color:#f92672>=</span> _data[Not(testindex), <span style=color:#f92672>:</span>] 
println(<span style=color:#e6db74>&#34;testdf: </span><span style=color:#e6db74>$</span>(nrow(testdf))<span style=color:#e6db74> traindf: </span><span style=color:#e6db74>$</span>(nrow(traindf))<span style=color:#e6db74>&#34;</span>)

<span style=color:#75715e># using the training data, set up vectors for glmnet</span>
y <span style=color:#f92672>=</span> map(x <span style=color:#f92672>-&gt;</span> x <span style=color:#f92672>==</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>?</span> <span style=color:#ae81ff>0</span> <span style=color:#f92672>:</span> <span style=color:#ae81ff>1</span>, traindf[<span style=color:#f92672>:</span>, <span style=color:#e6db74>:class</span>]) <span style=color:#75715e># convert outcmoe variable to 0/1</span>
X <span style=color:#f92672>=</span> <span style=color:#66d9ef>Matrix</span>(traindf[<span style=color:#f92672>:</span>, <span style=color:#ae81ff>3</span><span style=color:#f92672>:</span><span style=color:#ae81ff>11</span>])
path <span style=color:#f92672>=</span> glmnet(X, y)

path<span style=color:#f92672>.</span>a0 <span style=color:#75715e># intercept values</span>
path<span style=color:#f92672>.</span>betas <span style=color:#75715e># coefficient values</span>

<span style=color:#75715e># for each solution (i.e. for each lambda value), sum up the coefficient values for all 9 predictors </span>
<span style=color:#75715e># why plot against the maximum?</span>
betaNorm <span style=color:#f92672>=</span> [norm(x, <span style=color:#ae81ff>1</span>) <span style=color:#66d9ef>for</span> x <span style=color:#66d9ef>in</span> eachslice(path<span style=color:#f92672>.</span>betas,dims<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>)] <span style=color:#75715e># norm(_, 1) is sum</span>

<span style=color:#75715e>#@gp &#34;reset session&#34;</span>
<span style=color:#a6e22e>@gp</span> <span style=color:#e6db74>&#34;reset&#34;</span>
<span style=color:#a6e22e>@gp</span> <span style=color:#f92672>:-</span> <span style=color:#e6db74>&#34;set xlabel &#39;maximum beta (coefficient) for each predictor&#39;&#34;</span>
<span style=color:#a6e22e>@gp</span> <span style=color:#f92672>:-</span> <span style=color:#e6db74>&#34;set ylabel &#39;beta value over each parameter value&#39;&#34;</span>
<span style=color:#66d9ef>for</span> p <span style=color:#66d9ef>in</span> <span style=color:#ae81ff>1</span><span style=color:#f92672>:</span><span style=color:#ae81ff>9</span> 
<span style=color:#a6e22e>@gp</span> <span style=color:#f92672>:-</span> betaNorm path<span style=color:#f92672>.</span>betas<span style=color:#f92672>&#39;</span>[<span style=color:#f92672>:</span>, p] <span style=color:#e6db74>&#34;title &#39;var: </span><span style=color:#e6db74>$p</span><span style=color:#e6db74>&#39; with lines&#34;</span> <span style=color:#f92672>:-</span>
<span style=color:#66d9ef>end</span>
<span style=color:#a6e22e>@gp</span>

<span style=color:#75715e># To predict the output for each model along the path for a given set of predictors</span>
yt <span style=color:#f92672>=</span> map(x <span style=color:#f92672>-&gt;</span> x <span style=color:#f92672>==</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>?</span> <span style=color:#ae81ff>0</span> <span style=color:#f92672>:</span> <span style=color:#ae81ff>1</span>, testdf[<span style=color:#f92672>:</span>, <span style=color:#e6db74>:class</span>]) <span style=color:#75715e># convert outcmoe variable to 0/1</span>
Xt <span style=color:#f92672>=</span> <span style=color:#66d9ef>Matrix</span>(testdf[<span style=color:#f92672>:</span>, <span style=color:#ae81ff>3</span><span style=color:#f92672>:</span><span style=color:#ae81ff>11</span>])
predict(path, Xt)

<span style=color:#75715e># here each row represents the final output of the regression over all the lambda values </span>
<span style=color:#75715e># for row[1] with 56 columns means the output of y = ax1 + ax2 + ... where each value is a continous version of y </span>
<span style=color:#75715e># and we have 56 such guesses over the 56 lambda values </span>
<span style=color:#75715e># which lambda value is best? (i.e. which row to pick)</span>

<span style=color:#75715e># use the training set to do n-fold cross-validation. </span>
cv <span style=color:#f92672>=</span> glmnetcv(X, y)
minloss_idx <span style=color:#f92672>=</span> argmin(cv<span style=color:#f92672>.</span>meanloss)
coef(cv) <span style=color:#75715e># equivalent to cv.path.betas[:, minloss_idx]</span>

<span style=color:#75715e># now we know what lambda is best, pick that lambda and see the prediction accuracy on the testset</span>
yht <span style=color:#f92672>=</span> round<span style=color:#f92672>.</span>(predict(path, Xt, outtype <span style=color:#f92672>=</span> <span style=color:#e6db74>:prob</span>), digits<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>);
yht[<span style=color:#f92672>:</span>, minloss_idx] <span style=color:#75715e># get the prediction from the lambda parameter that is minimized</span>

<span style=color:#75715e># or alternatively, just predict using the cross fold result... they are equivalent. </span>
yht <span style=color:#f92672>=</span> round<span style=color:#f92672>.</span>(predict(cv, Xt, outtype <span style=color:#f92672>=</span> <span style=color:#e6db74>:prob</span>), digits<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>)

<span style=color:#75715e># Compare against the actual y values </span>
DataFrame(target<span style=color:#f92672>=</span>yt, predict<span style=color:#f92672>=</span>yht) <span style=color:#f92672>|&gt;</span> pretty_table

</code></pre></div><h2 id=other-norms>Other norms?</h2><p>While regularization is often with $L_1$ or $L_2$ norms, it could be done with other norms as well. For example, one may use $L_0$ which which simply counts the number of nonzero components of a vector (but actually isn&rsquo;t strictly a norm in the mathematical sense). However, this is not common. See answers and comments on <a href=https://stats.stackexchange.com/questions/269298/why-do-we-only-see-l-1-and-l-2-regularization-but-not-other-norms/269407>this</a></p><h3 id=reference>Reference</h3><p>The Elements of Statistical Learning by Trevor Hastie, Rob Tibshirani, Jerome Friedman. Link: <a href=https://web.stanford.edu/~hastie/ElemStatLearn/>https://web.stanford.edu/~hastie/ElemStatLearn/</a></p></div><div class=post-footer></div></article></main></body></html>